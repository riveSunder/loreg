{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Level Regularization Iteration Notebook\n",
    "A notebook for gaining intuition into 4 regularization techniques (l1 and l2 weight penalties, pruning, and dropout). Implemented at a low level and using the `scikit-learn` digits dataset, this notebook relies only on `numpy`, `scikit-learn`, and `matplotlib` for plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# use built in dataset from scikit-learn\n",
    "import sklearn.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def grad_from_sigz(sigz):\n",
    "    return sigz * (1-sigz)\n",
    "\n",
    "def stable_softmax(a):\n",
    "    # stabilize as in https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    a_ = a - np.max(a)\n",
    "    softmax = np.zeros_like(a)\n",
    "    \n",
    "    for ck in range(a_.shape[1]):\n",
    "        softmax[:,ck] = np.exp(a_[:,ck]) / (np.sum(np.exp(a_),axis=1)) \n",
    "    return softmax\n",
    "\n",
    "def accuracy(y_pred,y):\n",
    "    accuracy = np.sum(1.*np.argmax(y_pred,axis=1) == np.argmax(y,axis=1)) / np.sum(y)\n",
    "    return accuracy\n",
    "\n",
    "# random seed\n",
    "my_seed = 1\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "\n",
    "# ** Hyperparameters **\n",
    "learning_rate = 3e-3\n",
    "# regularization\n",
    "dropout_rate = 0.02500\n",
    "pruning_rate = 0.10 # prune weights with absolute value below this number of std deviations\n",
    "prune_period = 2500\n",
    "my_lambda_l1 = 0#1e-1#0.0 #1e-5\n",
    "my_lambda_l2 = 0#1e1 #0 #0.0 #1e-5\n",
    "# training duration and disp period\n",
    "max_steps = 15000\n",
    "disp_period = 400\n",
    "init_scale = 1e-2\n",
    "# ** **\n",
    "\n",
    "\n",
    "# load digits dataset 64 features 10 classes 1797 samples\n",
    "print(\"loading digits dataset\") \n",
    "[x,y] = datasets.load_digits(return_X_y=True)\n",
    "# normalize for unit variance\n",
    "x = (x-np.mean(x)) / np.std(x)\n",
    "print(\"input shape:\",x.shape,\"targets shape: \", y.shape)\n",
    "\n",
    "# convert to one hot labels for targets\n",
    "number_classes = np.max(y)+1\n",
    "number_entries = y.shape[0]\n",
    "onehot_y = np.zeros((number_entries,number_classes))\n",
    "for counter in range(number_entries):\n",
    "    onehot_y[counter,y[counter]] = 1\n",
    "\n",
    "y = onehot_y\n",
    "# split into training, validation, and test sets (60% training, 20% each to validation and test)\n",
    "my_seed = 1337\n",
    "\n",
    "num_val = round(0.2*x.shape[0])\n",
    "np.random.seed(my_seed)\n",
    "np.random.shuffle(x)\n",
    "np.random.seed(my_seed)\n",
    "np.random.shuffle(y)\n",
    "\n",
    "x_val = x[0:num_val,:]\n",
    "y_val = y[0:num_val,:]\n",
    "x_test = x[num_val:2*num_val,:]\n",
    "y_test = y[num_val:2*num_val,:]\n",
    "\n",
    "x = x[2*num_val:-1,:]\n",
    "y = y[2*num_val:-1,:]\n",
    "\n",
    "# get data dimensions\n",
    "dim_x = x.shape[1] # 256\n",
    "dim_y = y.shape[1]\n",
    "dim_h = 128\n",
    "\n",
    "\n",
    "# initialize weights\n",
    "wxh0 = init_scale*np.random.randn(dim_x,dim_h) \n",
    "whh1 = init_scale*np.random.randn(dim_h,dim_h)\n",
    "why = init_scale*np.random.randn(dim_h,dim_y)\n",
    "b0 = init_scale*np.random.randn()\n",
    "b1 = init_scale*np.random.randn()\n",
    "if(pruning_rate):\n",
    "    mask_wxh0,mask_whh1,mask_why = np.ones_like(wxh0), np.ones_like(whh1), np.ones_like(why)\n",
    "\n",
    "    \n",
    "for ck in range(max_steps+1):\n",
    "        \n",
    "    # forward pass\n",
    "    h0 = np.dot(x,wxh0) + b0\n",
    "    h0[h0<0] = 0. # apply relu\n",
    "    \n",
    "    h1 = np.dot(h0,whh1) + b1\n",
    "    #h1 = sigmoid(h1)\n",
    "    if(dropout_rate):\n",
    "        h1 *= (1.*np.random.random((h1.shape)) > dropout_rate)\n",
    "        #h1 /= (1-dropout_rate)\n",
    "    \n",
    "    y_pred = stable_softmax(np.dot(h1,why))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # cross entropy loss\n",
    "    loss = - (1/len(x)) * np.sum(y*np.log(y_pred)+(1-y)*np.log(1-y_pred))\n",
    "    \n",
    "    if(my_lambda_l1):\n",
    "        # L2 regularization: add weight penalty\n",
    "        loss += my_lambda_l1/len(x) * np.sum(np.abs(wxh0))+np.sum(np.abs(whh1))+np.sum(np.abs(why))\n",
    "    if(my_lambda_l2):\n",
    "        # L2 regularization: add weight penalty\n",
    "        loss += my_lambda_l2/(2*len(x)) * np.sum(wxh0**2)+np.sum(whh1**2)+np.sum(why**2)\n",
    "    \n",
    "            \n",
    "    if (ck % disp_period ==0 ): \n",
    "        # forward pass (validation data)\n",
    "        h0_val = np.dot(x_val,wxh0) + b0\n",
    "        h0_val[h0_val<0] = 0. # apply relu\n",
    "\n",
    "        h1_val = np.dot(h0_val,whh1) + b1\n",
    "        #h1_val = h1_val\n",
    "\n",
    "        y_pred_val = stable_softmax(np.dot(h1_val,why))\n",
    "        \n",
    "        train_acc = accuracy(y_pred,y)\n",
    "        val_acc = accuracy(y_pred_val,y_val)\n",
    "        print(\"CE loss step %i = %.3e, training/validation accuracy: %.3f/%.3f\"%(ck,loss,train_acc,val_acc))\n",
    "        if(0):\n",
    "            plt.figure(figsize=(8,8))\n",
    "            plt.hist([wxh0,whh1,why],bins=16)\n",
    "            plt.title(\"weights histogram\")\n",
    "            plt.show()\n",
    "    \n",
    "    #backward pass\n",
    "    dy = (y_pred-y) / len(x)\n",
    "    dwhy = np.dot(dy.T, h1).T # n by y n by h --> y by h\n",
    "    dh1 = np.dot(dy, why.T) # n by y , h by y\n",
    "    #dh1 = dh1 * grad_from_sigz(sigmoid(h1))\n",
    "    db1 = dh1\n",
    "    \n",
    "    dwhh1 = np.dot(dh1.T,h0).T\n",
    "    \n",
    "    \n",
    "    dh0 = np.dot(dh1,whh1.T)\n",
    "    dh0[h0<0] = 0 #backprop through relu\n",
    "    db0 = dh0\n",
    "    \n",
    "    dwxh0 = np.dot(dh0.T,x).T # n by h , n by x --> x by h\n",
    "    \n",
    "    if(my_lambda_l2):\n",
    "        # add the gradients for an l2 regularization term\n",
    "        for param, dparam in zip([wxh0,whh1,why],[dwxh0,dwhh1,dwhy]):\n",
    "            dparam += my_lambda_l2/len(x)*(param)\n",
    "            \n",
    "    if(my_lambda_l1):\n",
    "        # add the gradients for an l2 regularization term\n",
    "        for param, dparam in zip([wxh0,whh1,why],[dwxh0,dwhh1,dwhy]):\n",
    "            dparam += my_lambda_l1/len(x)*(np.sign(param))\n",
    "    \n",
    "    if(pruning_rate > 0.):\n",
    "        # remove weights below threshold absolute value\n",
    "        if(ck and ck % prune_period == 0):\n",
    "            print(\"pruning weights...\")\n",
    "            for param,param_mask in zip([wxh0,whh1,why],[mask_wxh0,mask_whh1,mask_why]):\n",
    "                \n",
    "                std_dev = np.std(param)\n",
    "                print(\"pruning {} of {} weights below threshold\".format(\\\n",
    "                        np.sum(np.abs(param) < pruning_rate*std_dev), np.sum(param == param)))\n",
    "                \n",
    "                param[np.abs(param) < pruning_rate*std_dev] = 0\n",
    "                \n",
    "    \n",
    "        # prevent learning on pruned connections\n",
    "        dwxh0 *= mask_wxh0\n",
    "        dwhh1 *= mask_whh1\n",
    "        dwhy *= mask_why\n",
    "    \n",
    "    \n",
    "    dwxh0[dwxh0>5.0],dwxh0[dwxh0<-5.0] = 5.,-5.\n",
    "    dwhh1[dwhh1>5.0],dwhh1[dwhh1<-5.0] = 5.,-5.\n",
    "    dwhy[dwhy>5],dwhy[dwhy<-5] = 5,-5\n",
    "    \n",
    "    for param, dparam in zip([wxh0,b0,whh1,b1,why],[dwxh0,db0,dwhh1,db1,dwhy]):\n",
    "        param -= learning_rate * dparam\n",
    "    \n",
    "    \n",
    "                \n",
    "            \n",
    "                \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
